# -*- coding: utf-8 -*-
"""credit card.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1392dfO5dG0zQyY2bLK0kChjNu4lfWenH
"""

# ===============================
# 1. Setup & Import Libraries
# ===============================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve

# ML Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# For imbalance check
from collections import Counter

# ===============================
# 2. Load Dataset
# ===============================
# Upload dataset manually if needed in Colab
from google.colab import files
uploaded = files.upload()

# Replace filename with your uploaded CSV
df = pd.read_csv("creditcard.csv")

print(df.shape)
print(df.head())

# ===============================
# 3. Explore Data
# ===============================
print(df.info())
print(df["Class"].value_counts())  # 0 = normal, 1 = fraud

# Check imbalance
sns.countplot(x="Class", data=df)
plt.title("Fraud vs Normal Transactions")
plt.show()

# Basic stats
print(df.describe())

# ===============================
# 4. Data Preprocessing
# ===============================
# Features and target
X = df.drop("Class", axis=1)
y = df["Class"]

# Standardize 'Amount' column
scaler = StandardScaler()
X["Amount"] = scaler.fit_transform(X["Amount"].values.reshape(-1, 1))

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

print("Train:", Counter(y_train), "Test:", Counter(y_test))

# ===============================
# 5. Handle Imbalance with SMOTE
# ===============================
sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print("Before SMOTE:", Counter(y_train))
print("After SMOTE:", Counter(y_train_res))

# ===============================
# 6. Define Helper for Training Models
# ===============================
def evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else y_pred

    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("ROC AUC:", roc_auc_score(y_test, y_prob))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# ===============================
# 7. Train with Different Models
# ===============================

# Logistic Regression
print("==== Logistic Regression ====")
evaluate_model(LogisticRegression(max_iter=1000), X_train_res, y_train_res, X_test, y_test)

# Decision Tree
print("==== Decision Tree ====")
evaluate_model(DecisionTreeClassifier(), X_train_res, y_train_res, X_test, y_test)

# Random Forest
print("==== Random Forest ====")
evaluate_model(RandomForestClassifier(n_estimators=100), X_train_res, y_train_res, X_test, y_test)

# Gradient Boosting
print("==== Gradient Boosting ====")
evaluate_model(GradientBoostingClassifier(), X_train_res, y_train_res, X_test, y_test)

# AdaBoost
print("==== AdaBoost ====")
evaluate_model(AdaBoostClassifier(), X_train_res, y_train_res, X_test, y_test)

# KNN
print("==== KNN ====")
evaluate_model(KNeighborsClassifier(n_neighbors=5), X_train_res, y_train_res, X_test, y_test)

# SVM (probability=True for ROC AUC)
print("==== SVM ====")
evaluate_model(SVC(probability=True), X_train_res, y_train_res, X_test, y_test)

# XGBoost
print("==== XGBoost ====")
evaluate_model(XGBClassifier(use_label_encoder=False, eval_metric="logloss"),
               X_train_res, y_train_res, X_test, y_test)

# ===============================
# 8. Feature Importance (Random Forest Example)
# ===============================
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train_res, y_train_res)

importances = pd.Series(rf.feature_importances_, index=X.columns)
importances.nlargest(10).plot(kind="barh")
plt.title("Top 10 Important Features")
plt.show()